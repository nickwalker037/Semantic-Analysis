{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Latent Dirichlet Allocation for Topic Modeling </h4>\n",
    "- is the most popular topic modeling technique\n",
    "- this assumes documents are produced from a mixture of topics, and those topics then generate words based on their probability distribution\n",
    "- given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a matrix factorization technique\n",
    "- In vector space, any corpus (collection of documents) can be represented as a document-term matrix\n",
    "\n",
    "- the following matrix shows a corpus of N documents (D1, D2, ..., DN) and a vocabulary size of M words (W1, W2, ..., WN)\n",
    "    - the value of i,j cell gives the frequency count of word Wj in Document Di\n",
    "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling2.png)\n",
    "\n",
    "- LDA converts this Document-Term Matrix into two lower dimensional matrices - M1 and M2\n",
    "    - M1 is a document-topics matrix and M2 is a topic-terms matrix with dimensions (N, K) and (K, M) respectively\n",
    "        - K is the number of topics\n",
    "        - M is the vocabulary size\n",
    "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/modeling3.png)\n",
    "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> These 2 matrices provide topic-word and document-topic distributions </h5>\n",
    "- But LDA improves upon this distribution by using sampling techniques to improve the matrices\n",
    "- It iterates through each word \"w\" for each document \"d\" and tries to adjust the current topic-word assignment with a new assignment\n",
    "    - a new topic \"k\" is assigned to word \"w\" with a probability P which is a product of two probabilities p1 and p2\n",
    "\n",
    "<h3> For every topic, two probabilities are calculated: </h3>\n",
    "- P1: p(topic t / document d)\n",
    "    - the proportion of words in document d that are currently assigned to the topic\n",
    "- P2: p(word w / topic t)\n",
    "    - the proportion of assignments to topic t over all documents that come from this word w\n",
    "    \n",
    "<h5> The current topic-word assignment is updated with a new topic with the probability, product of p1 and p2 </h5>\n",
    "- In this step, the model assumes that all the existing word-topic assignments except the current word are correct\n",
    "- this is essentially the probability that topic t generated word w, so it makes sense to adjust the current word's topic with new probability\n",
    "\n",
    "After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Parameters of LDA: </h2>\n",
    "\n",
    "<h6> Alpha and Beta Parameters </h6>\n",
    "- Alpha represents document-topic density\n",
    "    - higher value: documents are composed of more topics\n",
    "    - lower value: documents contain fewer topics\n",
    "- Beta represents topic-word density\n",
    "    - higher value: topics are composed of a large number of words in the corpus\n",
    "    - lower value: topics are composed of fewer words\n",
    "    \n",
    "<h6> Number of Topics </h6>\n",
    "- number of topics to be extracted from the corpus\n",
    "- example approache: Kullback Leibler Divergence Score\n",
    "\n",
    "<h6> Number of Topic Terms </h6>\n",
    "- Number of terms composed in a single topic\n",
    "- If you want to extract themes or concepts, higher number is better\n",
    "- If you want to extract features or terms, a lower number is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Example Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_all = [doc1, doc2, doc3, doc4, doc5] #compile documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Cleaning and Preprocessing </h3>\n",
    "- removing punctuations, stopwords, and normalizing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''. join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sugar bad consume sister like sugar father'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preparing Document-Term Matrix </h3>\n",
    "- all the text documents combined is known as the corpus\n",
    "- to run any mathematical model on text corpus, it is good practice to convert it into a matrix representation\n",
    "LDA models look for repeating patterns in the entire DT matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating the term dictionary for our corpus\n",
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting corpus into Document Term Matrix using above dictionary\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Running LDA Model </h3>\n",
    "- need to create an object for the LDA model and train it on the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA using gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running and training LDA matrix on document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.071*\"pressure\" + 0.041*\"feel\" + 0.041*\"school\"'), (1, '0.085*\"sister\" + 0.085*\"father\" + 0.084*\"sugar\"'), (2, '0.075*\"say\" + 0.075*\"lifestyle\" + 0.075*\"health\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))\n",
    "# each line is a topic with individual topic terms and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
