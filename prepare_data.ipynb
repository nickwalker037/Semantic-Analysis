{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'flags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a371116f72a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m tf.flags.DEFINE_integer(\n\u001b[0m\u001b[0;32m      2\u001b[0m   \"min_word_frequency\", 5, \"Minimum frequency of words in the vocabulary\")\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_sentence_len\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Maximum Sentence Length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'flags'"
     ]
    }
   ],
   "source": [
    "tf.flags.DEFINE_integer(\n",
    "  \"min_word_frequency\", 5, \"Minimum frequency of words in the vocabulary\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"max_sentence_len\", 160, \"Maximum Sentence Length\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "  \"input_dir\", os.path.abspath(\"./data\"),\n",
    "  \"Input directory containing original CSV data files (default = './data')\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "  \"output_dir\", os.path.abspath(\"./data\"),\n",
    "  \"Output directory for TFrEcord files (default = './data')\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "TRAIN_PATH = os.path.join(FLAGS.input_dir, \"train.csv\")\n",
    "VALIDATION_PATH = os.path.join(FLAGS.input_dir, \"valid.csv\")\n",
    "TEST_PATH = os.path.join(FLAGS.input_dir, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes words\n",
    "def tokenizer_fn(iterator):\n",
    "  return (x.split(\" \") for x in iterator)\n",
    "\n",
    "\n",
    "# returns an iterator over a CSV file and skips the header\n",
    "def create_csv_iter(filename):\n",
    "  with open(filename) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "      yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(input_iter, min_frequency):\n",
    "  \"\"\"\n",
    "  Creates and returns a VocabularyProcessor object with the vocabulary\n",
    "  for the input iterator.\n",
    "  \"\"\"\n",
    "  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      max_sentence_len,\n",
    "      min_frequency=min_frequency,\n",
    "      tokenizer_fn=tokenizer_fn)\n",
    "  vocab_processor.fit(input_iter)\n",
    "  return vocab_processor\n",
    "\n",
    "\n",
    "\n",
    "def transform_sentence(sequence, vocab_processor):\n",
    "  \"\"\"\n",
    "  Maps a single sentence into the integer vocabulary. Returns a python array.\n",
    "  \"\"\"\n",
    "  return next(vocab_processor.transform([sequence])).tolist()\n",
    "\n",
    "\n",
    "def create_text_sequence_feature(fl, sentence, sentence_len, vocab):\n",
    "  \"\"\"\n",
    "  Writes a sentence to FeatureList protocol buffer\n",
    "  \"\"\"\n",
    "  sentence_transformed = transform_sentence(sentence, vocab)\n",
    "  for word_id in sentence_transformed:\n",
    "    fl.feature.add().int64_list.value.extend([word_id])\n",
    "  return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_train(row, vocab):\n",
    "  \"\"\"\n",
    "  Creates a training example for the Ubuntu Dialog Corpus dataset.\n",
    "  Returnsthe a tensorflow.Example Protocol Buffer object.\n",
    "  \"\"\"\n",
    "  context, utterance, label = row\n",
    "  context_transformed = transform_sentence(context, vocab)\n",
    "  utterance_transformed = transform_sentence(utterance, vocab)\n",
    "  context_len = len(next(vocab._tokenizer([context])))\n",
    "  utterance_len = len(next(vocab._tokenizer([utterance])))\n",
    "  label = int(float(label))\n",
    "\n",
    "  # New Example\n",
    "  example = tf.train.Example()\n",
    "  example.features.feature[\"context\"].int64_list.value.extend(context_transformed)\n",
    "  example.features.feature[\"utterance\"].int64_list.value.extend(utterance_transformed)\n",
    "  example.features.feature[\"context_len\"].int64_list.value.extend([context_len])\n",
    "  example.features.feature[\"utterance_len\"].int64_list.value.extend([utterance_len])\n",
    "  example.features.feature[\"label\"].int64_list.value.extend([label])\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_test(row, vocab):\n",
    "  \"\"\"\n",
    "  Creates a test/validation example for the Ubuntu Dialog Corpus dataset.\n",
    "  Returnsthe a tensorflow.Example Protocol Buffer object.\n",
    "  \"\"\"\n",
    "  context, utterance = row[:2]\n",
    "  distractors = row[2:]\n",
    "  context_len = len(next(vocab._tokenizer([context])))\n",
    "  utterance_len = len(next(vocab._tokenizer([utterance])))\n",
    "  context_transformed = transform_sentence(context, vocab)\n",
    "  utterance_transformed = transform_sentence(utterance, vocab)\n",
    "\n",
    "  # New Example\n",
    "  example = tf.train.Example()\n",
    "  example.features.feature[\"context\"].int64_list.value.extend(context_transformed)\n",
    "  example.features.feature[\"utterance\"].int64_list.value.extend(utterance_transformed)\n",
    "  example.features.feature[\"context_len\"].int64_list.value.extend([context_len])\n",
    "  example.features.feature[\"utterance_len\"].int64_list.value.extend([utterance_len])\n",
    "\n",
    "  # Distractor sequences\n",
    "  for i, distractor in enumerate(distractors):\n",
    "    dis_key = \"distractor_{}\".format(i)\n",
    "    dis_len_key = \"distractor_{}_len\".format(i)\n",
    "    # Distractor Length Feature\n",
    "    dis_len = len(next(vocab._tokenizer([distractor])))\n",
    "    example.features.feature[dis_len_key].int64_list.value.extend([dis_len])\n",
    "    # Distractor Text Feature\n",
    "    dis_transformed = transform_sentence(distractor, vocab)\n",
    "    example.features.feature[dis_key].int64_list.value.extend(dis_transformed)\n",
    "  return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_file(input_filename, output_filename, example_fn):\n",
    "  \"\"\"\n",
    "  Creates a TFRecords file for the given input data and\n",
    "  example transformation function\n",
    "  \"\"\"\n",
    "  writer = tf.python_io.TFRecordWriter(output_filename)\n",
    "  print(\"Creating TFRecords file at {}...\".format(output_filename))\n",
    "  for i, row in enumerate(create_csv_iter(input_filename)):\n",
    "    x = example_fn(row)\n",
    "    writer.write(x.SerializeToString())\n",
    "  writer.close()\n",
    "  print(\"Wrote to {}\".format(output_filename))\n",
    "\n",
    "\n",
    "def write_vocabulary(vocab_processor, outfile):\n",
    "  \"\"\"\n",
    "  Writes the vocabulary to a file, one word per line.\n",
    "  \"\"\"\n",
    "  vocab_size = len(vocab_processor.vocabulary_)\n",
    "  with open(outfile, \"w\") as vocabfile:\n",
    "    for id in range(vocab_size):\n",
    "      word =  vocab_processor.vocabulary_._reverse_mapping[id]\n",
    "      vocabfile.write(word + \"\\n\")\n",
    "  print(\"Saved vocabulary to {}\".format(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filefolder = 'C:\\\\Users\\\\nicholas\\\\Desktop\\\\Python\\\\Chatbot\\\\data\\\\test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'C:\\\\Users\\\\nicholas\\\\Desktop\\\\Python\\\\Chatbot\\\\data\\\\train.csv'\n",
    "VALIDATION_PATH = 'C:\\\\Users\\\\nicholas\\\\Desktop\\\\Python\\\\Chatbot\\\\data\\\\valid.csv'\n",
    "TEST_PATH = 'C:\\\\Users\\\\nicholas\\\\Desktop\\\\Python\\\\Chatbot\\\\data\\\\test.csv'\n",
    "min_word_frequency = 5\n",
    "max_sentence_len = 160\n",
    "output_dir =  'C:\\\\Users\\\\nicholas\\\\Desktop\\\\Python\\\\Chatbot\\\\data2\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-53cf620fe94e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0minput_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_csv_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0minput_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_word_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total vocabulary size: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-d53d6f389a4b>\u001b[0m in \u001b[0;36mcreate_vocab\u001b[1;34m(input_iter, min_frequency)\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m   vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n\u001b[0m\u001b[0;32m      7\u001b[0m       \u001b[0mmax_sentence_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0mmin_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_frequency\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  print(\"Creating vocabulary...\")\n",
    "  input_iter = create_csv_iter(TRAIN_PATH)\n",
    "  input_iter = (x[0] + \" \" + x[1] for x in input_iter)\n",
    "  vocab = create_vocab(input_iter, min_frequency=min_word_frequency)\n",
    "  print(\"Total vocabulary size: {}\".format(len(vocab.vocabulary_)))\n",
    "\n",
    "  # Create vocabulary.txt file\n",
    "  write_vocabulary(\n",
    "    vocab, os.path.join(output_dir, \"vocabulary.txt\"))\n",
    "\n",
    "  # Save vocab processor\n",
    "  vocab.save(os.path.join(output_dir, \"vocab_processor.bin\"))\n",
    "\n",
    "  # Create validation.tfrecords\n",
    "  create_tfrecords_file(\n",
    "      input_filename=VALIDATION_PATH,\n",
    "      output_filename=os.path.join(output_dir, \"validation.tfrecords\"),\n",
    "      example_fn=functools.partial(create_example_test, vocab=vocab))\n",
    "\n",
    "  # Create test.tfrecords\n",
    "  create_tfrecords_file(\n",
    "      input_filename=TEST_PATH,\n",
    "      output_filename=os.path.join(output_dir, \"test.tfrecords\"),\n",
    "      example_fn=functools.partial(create_example_test, vocab=vocab))\n",
    "\n",
    "  # Create train.tfrecords\n",
    "  create_tfrecords_file(\n",
    "      input_filename=TRAIN_PATH,\n",
    "      output_filename=os.path.join(output_dir, \"train.tfrecords\"),\n",
    "      example_fn=functools.partial(create_example_train, vocab=vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
